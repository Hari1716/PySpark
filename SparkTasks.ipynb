{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcVC9mtloqzp",
    "outputId": "05058531-96f6-4716-f8c9-5b9c015a8134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|First Name|Last Name|Age|\n",
      "+----------+---------+---+\n",
      "|      John|      Doe| 28|\n",
      "|      Jane|    Smith| 35|\n",
      "|       Sam|    Brown| 50|\n",
      "+----------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Simple PySpark Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a simple DataFrame\n",
    "data = [(\"John\", \"Doe\", 28),\n",
    "        (\"Jane\", \"Smith\", 35),\n",
    "        (\"Sam\", \"Brown\", 50)]\n",
    "\n",
    "columns = [\"First Name\", \"Last Name\", \"Age\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "37Dzuz1kqtli",
    "outputId": "4be1e0ad-c269-450d-8949-1820677109a8"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\hp\\\\Downloads\\\\export.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5a4942d4e50d>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load the CSV into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"C:\\Users\\hp\\Downloads\\export.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Show the schema to ensure correct data types are inferred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\hp\\\\Downloads\\\\export.csv'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sum, count, desc\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"E-Commerce Analysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "# Assumes the file has been transferred using spark-submit --files \"C:\\Users\\hp\\Downloads\\export.csv\"\n",
    "#file_path = \"export.csv\"  # Use the filename directly as it's now available in the working directory\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\hp\\Downloads\\export.csv\")\n",
    "\n",
    "# Show the schema to ensure correct data types are inferred\n",
    "df.printSchema()\n",
    "\n",
    "# 1. Calculate total spending amount for each user\n",
    "total_spent_df = df.groupBy(\"user_id\").agg(sum(\"amount\").alias(\"total_spent\"))\n",
    "\n",
    "# 2. Calculate average transaction amount per user\n",
    "avg_transaction_df = df.groupBy(\"user_id\").agg(avg(\"amount\").alias(\"avg_transaction\"))\n",
    "\n",
    "# 3. Find the most frequently purchased category per user\n",
    "category_count_df = df.groupBy(\"user_id\", \"category\").agg(count(\"category\").alias(\"category_count\"))\n",
    "\n",
    "# Find the most frequently purchased category per user using a window function\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(desc(\"category_count\"))\n",
    "favorite_category_df = category_count_df.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "                                        .filter(col(\"rank\") == 1) \\\n",
    "                                        .select(\"user_id\", \"category\")\n",
    "\n",
    "# Join the three DataFrames together\n",
    "final_df = total_spent_df \\\n",
    "    .join(avg_transaction_df, on=\"user_id\", how=\"inner\") \\\n",
    "    .join(favorite_category_df, on=\"user_id\", how=\"inner\") \\\n",
    "    .withColumnRenamed(\"category\", \"favorite_category\")\n",
    "\n",
    "# Show the result\n",
    "final_df.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNY0tt4wsfie",
    "outputId": "a718bcdd-019c-4d56-ffb6-b30e7a00daf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/13 08:44:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Exception in thread \"main\" java.io.IOException: Illegal file pattern: error parsing regexp: invalid escape sequence: `\\U`\n",
      "\tat org.apache.hadoop.fs.GlobFilter.init(GlobFilter.java:71)\n",
      "\tat org.apache.hadoop.fs.GlobFilter.<init>(GlobFilter.java:50)\n",
      "\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:265)\n",
      "\tat org.apache.hadoop.fs.Globber.glob(Globber.java:202)\n",
      "\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2124)\n",
      "\tat org.apache.spark.util.DependencyUtils$.resolveGlobPath(DependencyUtils.scala:318)\n",
      "\tat org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2(DependencyUtils.scala:273)\n",
      "\tat org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2$adapted(DependencyUtils.scala:271)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.util.DependencyUtils$.resolveGlobPaths(DependencyUtils.scala:271)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$5(SparkSubmit.scala:393)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:393)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "Caused by: org.apache.hadoop.shaded.com.google.re2j.PatternSyntaxException: error parsing regexp: invalid escape sequence: `\\U`\n",
      "\tat org.apache.hadoop.shaded.com.google.re2j.Parser.parseEscape(Parser.java:1439)\n",
      "\tat org.apache.hadoop.shaded.com.google.re2j.Parser.parseInternal(Parser.java:966)\n",
      "\tat org.apache.hadoop.shaded.com.google.re2j.Parser.parse(Parser.java:802)\n",
      "\tat org.apache.hadoop.shaded.com.google.re2j.RE2.compileImpl(RE2.java:183)\n",
      "\tat org.apache.hadoop.shaded.com.google.re2j.Pattern.compile(Pattern.java:136)\n",
      "\tat org.apache.hadoop.shaded.com.google.re2j.Pattern.compile(Pattern.java:124)\n",
      "\tat org.apache.hadoop.fs.GlobPattern.set(GlobPattern.java:156)\n",
      "\tat org.apache.hadoop.fs.GlobPattern.<init>(GlobPattern.java:42)\n",
      "\tat org.apache.hadoop.fs.GlobFilter.init(GlobFilter.java:67)\n",
      "\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "!spark-submit --files \"C:\\Users\\hp\\Downloads\\export.csv\" --master local[*] e_commerce_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZZLVN4wolTx",
    "outputId": "98895dc0-8ddb-4d8a-dde8-e40715b0cb53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------------+---------------+\n",
      "|user_id|total_spent|avg_transaction|favorite_sector|\n",
      "+-------+-----------+---------------+---------------+\n",
      "|     U2|      230.0|          115.0|           Food|\n",
      "|     U1|      320.0|          160.0|      Education|\n",
      "|     U4|      360.0|          180.0|         Retail|\n",
      "|     U3|      340.0|          170.0|      Education|\n",
      "+-------+-----------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import date\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Sector Transactions Analysis\").getOrCreate()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"sector\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"transaction_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Create a new sample dataset with different sectors\n",
    "data = [\n",
    "    (\"T1\", \"U1\", \"P1\", \"Healthcare\", 200.0, date(2024, 1, 10)),\n",
    "    (\"T2\", \"U1\", \"P2\", \"Education\", 120.0, date(2024, 1, 14)),\n",
    "    (\"T3\", \"U2\", \"P3\", \"Retail\", 150.0, date(2024, 2, 3)),\n",
    "    (\"T4\", \"U2\", \"P4\", \"Food\", 80.0, date(2024, 2, 5)),\n",
    "    (\"T5\", \"U3\", \"P5\", \"Healthcare\", 250.0, date(2024, 3, 1)),\n",
    "    (\"T6\", \"U3\", \"P6\", \"Education\", 90.0, date(2024, 3, 3)),\n",
    "    (\"T7\", \"U4\", \"P7\", \"Retail\", 300.0, date(2024, 4, 7)),\n",
    "    (\"T8\", \"U4\", \"P8\", \"Food\", 60.0, date(2024, 4, 9))\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Calculate total spending amount and average transaction amount per user\n",
    "spending_summary_df = df.groupBy(\"user_id\").agg(\n",
    "    F.sum(\"amount\").alias(\"total_spent\"),\n",
    "    F.avg(\"amount\").alias(\"avg_transaction\")\n",
    ")\n",
    "\n",
    "# Calculate the most frequently purchased sector for each user\n",
    "sector_count_df = df.groupBy(\"user_id\", \"sector\") \\\n",
    "    .count() \\\n",
    "    .withColumn(\"rank\", F.row_number().over(\n",
    "        Window.partitionBy(\"user_id\").orderBy(F.desc(\"count\"))\n",
    "    )) \\\n",
    "    .filter(F.col(\"rank\") == 1) \\\n",
    "    .select(\"user_id\", F.col(\"sector\").alias(\"favorite_sector\"))\n",
    "\n",
    "# Join the summary and favorite sector results\n",
    "final_result_df = spending_summary_df.join(sector_count_df, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# Show the result\n",
    "final_result_df.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
